The objective of this project is to analysis Classification (or clustering) and Finding rules by using Apriori and describe classification using Decision tree, Na√Øve Bayes, SVM. We have shown some comparison between those classification and reach on a result to choose which parts or take the best accuracy model as shown.  

Dataset

  The dataset used in this study is from Kaggle. The dataset has 11 attributes and 400 rows. Each row corresponds to one particular Customer and each attribute corresponds to the observations or type of each patient.  
  
Apriori
  
  Apriori uses a "bottom up" approach, where frequent subsets are extended one item at a time (a step known as candidate generation, and groups of candidates are tested against the data. The apriori principle can reduce the number of item sets we need to examine. Put simply, the apriori principle states that if an item set is infrequent, then all its subsets must also be infrequent.  

Decision Tree

  Decision tree learning uses a decision tree as a predictive model which maps input about an item to output of the item. Tree models with finite classes of output are called classification trees. In these tree structures, leaves represent class labels and branches represent relation between attributes that results in those class labels. Decision trees with continuous output classes are called regression trees. In data mining, a decision tree can be an input for decision making.
  
Support Vector Machine

  The SVM is a supervised machine learning algorithm for margin classification. It puts a hyperplane between the classes. SVM performs classification tasks by maximizing the margin which separates the classes while minimizing the classification errors.  

Naive Bayes

  The Naive Bayes algorithm represents a supervised machine learning method for classification. It uses a probabilistic model by determining probabilities of the outcomes. It is used in diagnostic and predictive problems. Naive Bayes is robust to noise in input dataset.
